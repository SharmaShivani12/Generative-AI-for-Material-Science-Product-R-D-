{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Through this notebook, I am trying to share my understanding of:\n\nHow generative AI models, such as Stable Diffusion XL, can be guided through prompt engineering to generate scientifically meaningful images rather than artistic outputs.\n\nHow carefully designed prompts can approximate microscopy-style material representations, enabling synthetic data generation for material characterization, defect detection, and segmentation tasks.\n\nThe importance of controlling visual factors such as illumination, contrast, depth of field, and structural regularity to produce data suitable for computer vision pipelines.\n\nHow synthetic images can be leveraged in industrial R&D contexts to supplement limited experimental data and accelerate model development.\n\n## üåà Introduction to Diffusion Models\n\nDiffusion models are a powerful class of **generative AI models** used to create data such as üñºÔ∏è images, üéµ audio, and other complex signals. Their core idea is simple yet elegant: **learn how to turn noise into meaningful data**.\n\n---\n\n### üîÅ How Do Diffusion Models Work?\n\nThey operate in two main phases:\n\n1. **üß™ Forward Process (Noising)**\n   - Gradually add random noise to real data\n   - After many steps, the data becomes pure noise\n\n2. **üß† Reverse Process (Denoising)**\n   - A neural network learns how to remove noise step by step\n   - Starting from noise, it reconstructs realistic data\n\nDuring generation, the model begins with random noise and repeatedly denoises it until a clean sample appears ‚ú®\n\n---\n\n### üöÄ Why Diffusion Models Are Popular\n- üåü High-quality and detailed outputs  \n- üßò Stable training compared to GANs  \n- üß© Extremely flexible and controllable  \n\n---\n\n### üõ†Ô∏è Common Applications\n- üñºÔ∏è Image generation (text-to-image, image-to-image)\n- ‚úèÔ∏è Image editing (inpainting, super-resolution)\n- üé∂ Audio and music generation\n- üè• Medical imaging\n- üß¨ Molecule and protein design\n- üìä Data augmentation for ML models\n\n---\n\n### üß† In Short\n> Diffusion models generate realistic data by **slowly refining noise into structure**, step by step.\n\nüéØ This approach has become the backbone of many modern generative AI systems.\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install -q diffusers transformers accelerate safetensors torch torchvision xformers\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport gc\nimport os\nfrom diffusers import StableDiffusionXLPipeline\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nimport os\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:53:35.995314Z","iopub.execute_input":"2025-12-19T21:53:35.995627Z","iopub.status.idle":"2025-12-19T21:53:39.539566Z","shell.execute_reply.started":"2025-12-19T21:53:35.995604Z","shell.execute_reply":"2025-12-19T21:53:39.538737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Load Stable Diffusion XL","metadata":{}},{"cell_type":"code","source":"model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n\npipe = StableDiffusionXLPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True\n)\n\npipe = pipe.to(\"cuda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:53:47.417852Z","iopub.execute_input":"2025-12-19T21:53:47.418833Z","iopub.status.idle":"2025-12-19T21:53:50.747243Z","shell.execute_reply.started":"2025-12-19T21:53:47.41878Z","shell.execute_reply":"2025-12-19T21:53:50.746613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.   Optimize for Kaggle GPU Memory","metadata":{}},{"cell_type":"code","source":"pipe.enable_xformers_memory_efficient_attention()\npipe.enable_attention_slicing()\npipe.enable_vae_slicing()\npipe.enable_sequential_cpu_offload()\n\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:53:54.937891Z","iopub.execute_input":"2025-12-19T21:53:54.938606Z","iopub.status.idle":"2025-12-19T21:54:02.031892Z","shell.execute_reply.started":"2025-12-19T21:53:54.938579Z","shell.execute_reply":"2025-12-19T21:54:02.031265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîπ What Is Happening in This Step \n\nIn this step, we use a **diffusion model pipeline** to generate synthetic images based on text descriptions (**prompts**).\n\n---\n\n### üìù Prompt Definition\nEach prompt describes a **specific material microstructure** (e.g., porous, cracked, fibrous) along with a fixed visual style.  \nThis text acts as a **conditioning signal** that guides the image generation process.\n\nUsing multiple prompts allows us to generate a **batch of diverse images** in a single run.\n\n---\n\n### üß† Image Generation\nThe diffusion pipeline starts from **random noise** and progressively removes noise over multiple steps, shaping the image to match the given prompt.\n\nKey parameters:\n- **`num_inference_steps = 35`**  \n  Controls how many denoising steps are used. More steps generally improve image quality.\n- **`guidance_scale = 7.5`**  \n  Controls how strongly the model follows the prompt. A moderate value balances realism and diversity.\n- **`negative_prompt`**  \n  Specifies features to avoid, helping reduce artifacts and unrealistic textures.\n\n---\n\n### üñºÔ∏è Output\nThe `.images` attribute extracts the final generated images from the pipeline output.\n\n---\n\n### üéØ Why This Step Matters\nThis step enables the creation of **high-quality synthetic microstructure images** that are:\n- Prompt-aligned\n- Style-consistent\n- Useful for dataset augmentation, visualization, or downstream ML tasks\n\n---\n","metadata":{}},{"cell_type":"code","source":"BASE_STYLE = (\n    \"scientific microscopy visualization, research-grade image, \"\n    \"high-resolution scanning electron microscopy (SEM) style, \"\n    \"porous polymer material, interconnected closed-cell microstructure, \"\n    \"smooth rounded pores, thin pore walls with clearly defined boundaries, \"\n    \"uniform pore size distribution, scientific material characterization, \"\n    \"industrial R&D quality inspection, neutral color palette, \"\n    \"flat uniform illumination, high contrast, high depth of field, \"\n    \"sharp focus across entire image, no artistic stylization\"\n)\n\n\nNEGATIVE_PROMPT = (\n    \"artistic, cartoon, painting, illustration, watermark, \"\n    \"text, blurry, low resolution\"\n)\n\nPROMPT = (\n    \"porous polymer microstructure with surface defects, \"\n    \"micro cracks and voids, industrial quality inspection, \"\n    + BASE_STYLE\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:58:08.555182Z","iopub.execute_input":"2025-12-19T21:58:08.556114Z","iopub.status.idle":"2025-12-19T21:58:08.560153Z","shell.execute_reply.started":"2025-12-19T21:58:08.556087Z","shell.execute_reply":"2025-12-19T21:58:08.559371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîç What This Prompt Does ?\n\nEach phrase in the prompt is intentionally chosen to constrain the generative model toward realistic, industrial imagery:\n\n- Scientific microscopy / SEM style\nEncourages textures and structures similar to real microscopy data.\n\n- Porous polymer material & closed-cell microstructure\nDefines the material's physical structure.\n\n- Thin pore walls & defined boundaries\nImproves suitability for semantic segmentation and defect detection.\n\n- Uniform illumination & neutral color palette\nReduces artistic lighting artifacts and improves consistency.\n\n- High depth of field & sharp focus\nEnsures the entire image is usable for quantitative analysis.\n\n- No artistic stylization\nSuppresses artistic bias commonly present in diffusion models.\n\n### üè≠ Why This Matters for Industrial R&D\n\n\nThis prompt design enables:\n\n* Synthetic data generation for material science research\n\n* Training and benchmarking of segmentation models\n\n* Simulation of surface and structural defects\n\n* Faster experimentation when real microscopy data is limited\n\n### ‚ö†Ô∏è Prompt Design Note\n\nOver-constraining prompts can reduce structural diversity.\nFor experimentation, prompts can be modularized into:\n\n* Style constraints (microscopy, illumination)\n\n* Structural constraints (porosity, defects)\n\n* Material context (polymer, surface quality)\n\nThis allows controlled variation while maintaining realism.","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\n\nimage = pipe(\n    prompt=PROMPT,\n    negative_prompt=NEGATIVE_PROMPT,\n    num_inference_steps=25,   # safe for Kaggle\n    guidance_scale=7.0,\n    height=768,\n    width=768\n).images[0]\n\nimage\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:58:12.000547Z","iopub.execute_input":"2025-12-19T21:58:12.001154Z","iopub.status.idle":"2025-12-19T21:59:27.405036Z","shell.execute_reply.started":"2025-12-19T21:58:12.001131Z","shell.execute_reply":"2025-12-19T21:59:27.404209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"synthetic_material_data\", exist_ok=True)\nimage.save(\"synthetic_material_data/material_defect.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:59:45.690488Z","iopub.execute_input":"2025-12-19T21:59:45.690782Z","iopub.status.idle":"2025-12-19T21:59:45.918297Z","shell.execute_reply.started":"2025-12-19T21:59:45.690761Z","shell.execute_reply":"2025-12-19T21:59:45.917699Z"}},"outputs":[],"execution_count":null}]}